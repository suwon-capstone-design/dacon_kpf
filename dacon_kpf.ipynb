{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413003e2-7919-4117-b344-09c284c2612e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: seaborn in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\als77\\appdata\\roaming\\python\\python312\\site-packages (1.9.4)\n",
      "Requirement already satisfied: kiwipiepy in c:\\users\\als77\\appdata\\roaming\\python\\python312\\site-packages (0.21.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyarrow in c:\\programdata\\anaconda3\\lib\\site-packages (14.0.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: kiwipiepy_model<0.22,>=0.21 in c:\\users\\als77\\appdata\\roaming\\python\\python312\\site-packages (from kiwipiepy) (0.21.0)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from kiwipiepy) (4.66.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->kiwipiepy) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!pip install pandas matplotlib seaborn wordcloud kiwipiepy scikit-learn pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c7d5e-a936-4348-b18f-75f15714e64f",
   "metadata": {},
   "source": [
    "# ë‰´ìŠ¤ ê¸°ì‚¬ í†µí•© ë¶„ì„ ë¦¬í¬íŠ¸\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ë‰´ìŠ¤ ê¸°ì‚¬ì˜ ì—¬ëŸ¬ ì§€í‘œì™€ ë…ì ì¸êµ¬ í†µê³„, ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš©ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¶„ì„ íŒŒì´í”„ë¼ì¸:**\n",
    "1.  **ê¸°ì‚¬ ì§€í‘œ ë¶„ì„**: ìƒìœ„ ê¸°ì‚¬ì˜ í•µì‹¬ ì§€í‘œ ì‹œê°í™”\n",
    "2.  **ì¸êµ¬í†µê³„ ë¶„ì„**: ìƒìœ„ ê¸°ì‚¬ì˜ ë…ìì¸µ ë¶„í¬ ì‹œê°í™”\n",
    "3.  **í‚¤ì›Œë“œ ë¶„ì„**: ì›Œë“œí´ë¼ìš°ë“œë¥¼ í†µí•œ í•µì‹¬ í‚¤ì›Œë“œ ë„ì¶œ\n",
    "4.  **í† í”½ ëª¨ë¸ë§**: LDAë¥¼ í™œìš©í•œ ê¸°ì‚¬ ì£¼ì œ êµ°ì§‘í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43605057-86e2-44a7-b4af-aa8877a67942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ Import\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from kiwipiepy import Kiwi\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9929a71-38ae-4659-bc19-8ed9f024e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜\n",
    "\n",
    "\"\"\"\n",
    "ê³µí†µ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ëª¨ë“ˆ\n",
    "- í•œê¸€ í°íŠ¸ ì„¤ì •\n",
    "- ë°ì´í„° ë¡œë“œ ìµœì í™”\n",
    "- ìƒìœ„ ê¸°ì‚¬ ì¶”ì¶œ\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def set_korean_font():\n",
    "    \"\"\"ìš´ì˜ì²´ì œì— ë§ëŠ” í•œê¸€ í°íŠ¸ë¥¼ ì„¤ì •í•˜ê³ , í°íŠ¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    font_name = ''\n",
    "    try:\n",
    "        if platform.system() == 'Windows':\n",
    "            font_name = 'Malgun Gothic'\n",
    "            plt.rc('font', family=font_name)\n",
    "        elif platform.system() == 'Darwin':  # Mac OS\n",
    "            font_name = 'AppleGothic'\n",
    "            plt.rc('font', family=font_name)\n",
    "        else:  # Linux\n",
    "            font_name = 'NanumGothic'\n",
    "            plt.rc('font', family=font_name)\n",
    "\n",
    "        # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        print(f\"ì„±ê³µ: '{font_name}' í°íŠ¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        return font_name\n",
    "    except:\n",
    "        print(\"í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ì˜ í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_font_path():\n",
    "    \"\"\"ì›Œë“œí´ë¼ìš°ë“œìš© í°íŠ¸ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "    if platform.system() == 'Windows':\n",
    "        return 'c:/Windows/Fonts/malgun.ttf'\n",
    "    elif platform.system() == 'Darwin':\n",
    "        return '/System/Library/Fonts/Supplemental/AppleGothic.ttf'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_optimized_data(excel_path):\n",
    "    \"\"\"\n",
    "    Feather íŒŒì¼ì´ ìˆìœ¼ë©´ ìš°ì„  ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ Excel íŒŒì¼ì„ ë¡œë“œ í›„ Featherë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    feather_path = excel_path.replace('.xlsx', '.feather')\n",
    "    try:\n",
    "        df = pd.read_feather(feather_path)\n",
    "        print(f\"ì„±ëŠ¥ ìµœì í™”: '{feather_path}' íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ìµœì´ˆ ì‹¤í–‰: '{excel_path}' íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. ë‹¤ìŒ ì‹¤í–‰ë¶€í„°ëŠ” ì†ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.\")\n",
    "        df = pd.read_excel(excel_path)\n",
    "        # Feather í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ë¹ ë¥¸ ë¡œë“œë¥¼ ìœ„í•´)\n",
    "        try:\n",
    "            df.to_feather(feather_path)\n",
    "        except:\n",
    "            # Feather ì €ì¥ ì‹¤íŒ¨ì‹œ ê·¸ëƒ¥ ì§„í–‰\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_top_articles(metrics_df, top_n=20):\n",
    "    \"\"\"\n",
    "    ì •ê·œí™”ëœ ì¢…í•© ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ Nê°œì˜ article_id ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    metrics_df : DataFrame\n",
    "        article_metrics_monthly ë°ì´í„°\n",
    "    top_n : int\n",
    "        ì¶”ì¶œí•  ìƒìœ„ ê¸°ì‚¬ ê°œìˆ˜\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list : ìƒìœ„ Nê°œ article_id ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "    summary = metrics_df.groupby('article_id').agg({\n",
    "        'comments': 'sum',\n",
    "        'likes': 'sum',\n",
    "        'views_total': 'sum'\n",
    "    }).reset_index()\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    cols_to_scale = ['comments', 'likes', 'views_total']\n",
    "    summary[cols_to_scale] = scaler.fit_transform(summary[cols_to_scale])\n",
    "\n",
    "    # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ëŒ“ê¸€ 50%, ì¢‹ì•„ìš” 30%, ì¡°íšŒìˆ˜ 20%)\n",
    "    summary['score'] = (summary['comments'] * 0.5 +\n",
    "                        summary['likes'] * 0.3 +\n",
    "                        summary['views_total'] * 0.2)\n",
    "\n",
    "    top_articles = summary.sort_values(by='score', ascending=False).head(top_n)\n",
    "    # ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜ (demographics ë°ì´í„°ì™€ ì¼ì¹˜ì‹œí‚¤ê¸° ìœ„í•´)\n",
    "    return top_articles['article_id'].astype(str).tolist()\n",
    "\n",
    "\n",
    "def get_stopwords():\n",
    "    \"\"\"ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
    "\n",
    "    return [\n",
    "        'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ìœ„í•´', 'í†µí•´', 'ëŒ€í•œ', 'ë•Œë¬¸',\n",
    "        'ê´€ë ¨', 'ê°€ì¥', 'ì§€ë‚œ', 'ìµœê·¼', 'ë°”ë¡œ', 'ê²½ìš°', 'ì¤‘', 'ì†',\n",
    "        'ê¸°ì', 'ê¸°ì‚¬', 'ë‰´ìŠ¤', 'ì–¸ë¡ ì‚¬', 'ì–¸ë¡ ', 'ì·¨ì¬', 'ì‚¬ëŒ', 'ìš°ë¦¬', 'ìƒê°',\n",
    "        'ì½˜í…ì¸ ', 'ë¯¸ë””ì–´', 'ë°©ì†¡', 'ì‚¬íšŒ'\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c83ddc80-e6f7-405e-a773-3d6371b0f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NewsAnalyzer í´ë˜ìŠ¤ ì •ì˜\n",
    "\n",
    "\"\"\"\n",
    "ë‰´ìŠ¤ ê¸°ì‚¬ í†µí•© ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸\n",
    "1. ê¸°ì‚¬ ì§€í‘œ ë¶„ì„ (ì¡°íšŒìˆ˜, ì¢‹ì•„ìš”, ëŒ“ê¸€, ì¢…í•©ì ìˆ˜)\n",
    "2. ì¸êµ¬í†µê³„ ë¶„ì„ (ì—°ë ¹ëŒ€/ì„±ë³„)\n",
    "3. í‚¤ì›Œë“œ ë¶„ì„ (ì›Œë“œí´ë¼ìš°ë“œ)\n",
    "4. í† í”½ ëª¨ë¸ë§ (LDA)\n",
    "\"\"\"\n",
    "\n",
    "class NewsAnalyzer:\n",
    "    \"\"\"ë‰´ìŠ¤ ê¸°ì‚¬ ë¶„ì„ í´ë˜ìŠ¤\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir='./data/'):\n",
    "        \"\"\"ì´ˆê¸°í™” ë° ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.result_dir = './result/'\n",
    "        self.font_name = set_korean_font()\n",
    "        self.font_prop = {'family': self.font_name} if self.font_name else {}\n",
    "        self.top_n = 20  # ë¶„ì„í•  ìƒìœ„ ê¸°ì‚¬ ê°œìˆ˜\n",
    "        self.kiwi = Kiwi()\n",
    "\n",
    "        if not os.path.exists(self.result_dir):\n",
    "            os.makedirs(self.result_dir)\n",
    "        \n",
    "        # ë°ì´í„° ë¡œë“œ\n",
    "        self._load_data()\n",
    "\n",
    "        # ìƒìœ„ Nê°œ ê¸°ì‚¬ ì¶”ì¶œ\n",
    "        self.top_article_ids = get_top_articles(self.metrics_df, self.top_n)\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"ë°ì´í„° íŒŒì¼ ë¡œë“œ\"\"\"\n",
    "        try:\n",
    "            self.metrics_df = load_optimized_data(f'{self.data_dir}article_metrics_monthly.xlsx')\n",
    "            self.contents_df = pd.read_excel(f'{self.data_dir}contents.xlsx')\n",
    "            self.demo_part1 = load_optimized_data(f'{self.data_dir}demographics_part001.xlsx')\n",
    "            self.demo_part2 = load_optimized_data(f'{self.data_dir}demographics_part002.xlsx')\n",
    "\n",
    "            self.metrics_df['article_id'] = self.metrics_df['article_id'].astype(str)\n",
    "            self.contents_df['article_id'] = self.contents_df['article_id'].astype(str)\n",
    "            self.demo_part1['article_id'] = self.demo_part1['article_id'].astype(str)\n",
    "            self.demo_part2['article_id'] = self.demo_part2['article_id'].astype(str)\n",
    "            print(\"ëª¨ë“  ë°ì´í„° íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\\n\")\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Error: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {e.filename}\")\n",
    "            exit()\n",
    "\n",
    "    def analyze_metrics(self):\n",
    "        \"\"\"1. ê¸°ì‚¬ ì§€í‘œ ë¶„ì„ (ì›ë³¸ + ì •ê·œí™”)\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"1. ê¸°ì‚¬ ì§€í‘œ ë¶„ì„ ì‹œì‘\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # ê¸°ì‚¬ ID ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„° í•©ì‚°\n",
    "        article_summary = self.metrics_df.groupby('article_id').agg({\n",
    "            'comments': 'sum',\n",
    "            'likes': 'sum',\n",
    "            'views_total': 'sum'\n",
    "        }).reset_index()\n",
    "\n",
    "        # ì›ë³¸ ë°ì´í„° ë¶„ì„\n",
    "        self._analyze_and_plot(article_summary, is_normalized=False)\n",
    "\n",
    "        # ì •ê·œí™” ë°ì´í„° ë¶„ì„\n",
    "        self._analyze_and_plot(article_summary, is_normalized=True)\n",
    "\n",
    "        print(\"ê¸°ì‚¬ ì§€í‘œ ë¶„ì„ ì™„ë£Œ\\n\")\n",
    "\n",
    "    def _analyze_and_plot(self, data_df, is_normalized=False):\n",
    "        \"\"\"ì§€í‘œë³„ ìƒìœ„ Nê°œ ë§‰ëŒ€ ê·¸ë˜í”„ ìƒì„±\"\"\"\n",
    "        # ë°ì´í„° ë³µì‚¬ë³¸ ìƒì„±\n",
    "        summary_df = data_df.copy()\n",
    "\n",
    "        # ì •ê·œí™” ì˜µì…˜ ì²˜ë¦¬\n",
    "        if is_normalized:\n",
    "            scaler = MinMaxScaler()\n",
    "            cols_to_scale = ['comments', 'likes', 'views_total']\n",
    "            summary_df[cols_to_scale] = scaler.fit_transform(summary_df[cols_to_scale])\n",
    "            title_suffix = \"(ì •ê·œí™” ë°ì´í„°)\"\n",
    "            xlabel_suffix = \" (ì •ê·œí™”)\"\n",
    "            filename_suffix = \"normalized\"\n",
    "        else:\n",
    "            title_suffix = \"(ì›ë³¸ ë°ì´í„°)\"\n",
    "            xlabel_suffix = \"\"\n",
    "            filename_suffix = \"raw\"\n",
    "\n",
    "        # ê°€ì¤‘ì¹˜ ì ìˆ˜ ê³„ì‚°\n",
    "        summary_df['score'] = (summary_df['comments'] * 0.5 +\n",
    "                               summary_df['likes'] * 0.3 +\n",
    "                               summary_df['views_total'] * 0.2)\n",
    "        summary_df['article_id'] = summary_df['article_id'].astype(str)\n",
    "\n",
    "        # ê° ì§€í‘œë³„ ìƒìœ„ Nê°œ ë°ì´í„° ì¶”ì¶œ\n",
    "        top_data = {\n",
    "            'views': summary_df.sort_values(by='views_total', ascending=False).head(self.top_n),\n",
    "            'likes': summary_df.sort_values(by='likes', ascending=False).head(self.top_n),\n",
    "            'comments': summary_df.sort_values(by='comments', ascending=False).head(self.top_n),\n",
    "            'score': summary_df.sort_values(by='score', ascending=False).head(self.top_n)\n",
    "        }\n",
    "\n",
    "        # ê·¸ë˜í”„ ì‹œê°í™”\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "        fig.suptitle(f'ì‹ ë¬¸ê³¼ë°©ì†¡ ê¸°ì‚¬ í•µì‹¬ ì§€í‘œ ìƒìœ„ TOP {self.top_n} {title_suffix}',\n",
    "                     fontsize=20, y=1.02)\n",
    "\n",
    "        # ê³µí†µ ì†ì„±ì„ ë”•ì…”ë„ˆë¦¬ë¡œ ê´€ë¦¬\n",
    "        plot_configs = {\n",
    "            'views': {'ax': axes[0, 0], 'x': 'views_total', 'xlabel': f'ì´ ì¡°íšŒìˆ˜{xlabel_suffix}', 'palette': 'viridis'},\n",
    "            'likes': {'ax': axes[0, 1], 'x': 'likes', 'xlabel': f'ì´ ì¢‹ì•„ìš” ìˆ˜{xlabel_suffix}', 'palette': 'plasma'},\n",
    "            'comments': {'ax': axes[1, 0], 'x': 'comments', 'xlabel': f'ì´ ëŒ“ê¸€ ìˆ˜{xlabel_suffix}', 'palette': 'magma'},\n",
    "            'score': {'ax': axes[1, 1], 'x': 'score', 'xlabel': 'ì¢…í•© ì ìˆ˜', 'palette': 'cividis'}\n",
    "        }\n",
    "\n",
    "        for key, config in plot_configs.items():\n",
    "            data = top_data[key]\n",
    "            sns.barplot(ax=config['ax'], x=config['x'], y='article_id', data=data,\n",
    "                        order=data['article_id'], palette=config['palette'],\n",
    "                        hue='article_id', legend=False)\n",
    "            config['ax'].set_title(f'{config[\"xlabel\"].split(\" (\")[0]} ìƒìœ„ TOP {self.top_n}')\n",
    "            config['ax'].set_xlabel(config['xlabel'])\n",
    "            config['ax'].set_ylabel('ê¸°ì‚¬ ID' if config['ax'] in [axes[0, 0], axes[1, 0]] else '')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "        plt.savefig(f'{self.result_dir}article_metrics_summary_{filename_suffix}.png', dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"  - {self.result_dir}article_metrics_summary_{filename_suffix}.png ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "    def analyze_demographics(self):\n",
    "        \"\"\"2. ì¸êµ¬í†µê³„ ë¶„ì„ (ì—°ë ¹ëŒ€/ì„±ë³„)\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"2. ì¸êµ¬í†µê³„ ë¶„ì„ ì‹œì‘\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # ì¸êµ¬í†µê³„ ë°ì´í„° í•©ì¹˜ê¸° ë° ì „ì²˜ë¦¬\n",
    "        demographics_df = pd.concat([self.demo_part1, self.demo_part2], ignore_index=True)\n",
    "        demographics_df['article_id'] = demographics_df['article_id'].astype(str)\n",
    "\n",
    "        # ìƒìœ„ Nê°œ ê¸°ì‚¬ì˜ ì¸êµ¬í†µê³„ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "        top_demographics = demographics_df[\n",
    "            demographics_df['article_id'].isin(self.top_article_ids)\n",
    "        ].copy()\n",
    "\n",
    "        # 1) ëˆ„ì  ë§‰ëŒ€ ê·¸ë˜í”„ (ì „ì²´ í¬í•¨)\n",
    "        self._plot_demographics_bar(top_demographics.copy())\n",
    "\n",
    "        # 2) íˆíŠ¸ë§µ (ì „ì²´ í¬í•¨)\n",
    "        self._plot_demographics_heatmap(top_demographics.copy(), exclude_total=False)\n",
    "\n",
    "        # 3) íˆíŠ¸ë§µ (ì „ì²´ ì œì™¸)\n",
    "        self._plot_demographics_heatmap(top_demographics.copy(), exclude_total=True)\n",
    "\n",
    "        print(\"ì¸êµ¬í†µê³„ ë¶„ì„ ì™„ë£Œ\\n\")\n",
    "\n",
    "    def _plot_demographics_bar(self, top_demographics):\n",
    "        \"\"\"ì¸êµ¬í†µê³„ ëˆ„ì  ë§‰ëŒ€ ê·¸ë˜í”„\"\"\"\n",
    "        top_demographics['demographic_group'] = (\n",
    "                top_demographics['age_group'] + \" \" + top_demographics['gender']\n",
    "        )\n",
    "\n",
    "        demographics_summary = top_demographics.groupby(\n",
    "            ['article_id', 'demographic_group']\n",
    "        )['ratio'].mean().reset_index()\n",
    "\n",
    "        # ë°ì´í„° í”¼ë²—\n",
    "        pivot_df = demographics_summary.pivot(\n",
    "            index='article_id', columns='demographic_group', values='ratio'\n",
    "        ).fillna(0)\n",
    "        pivot_df = pivot_df.loc[self.top_article_ids]\n",
    "\n",
    "        # ì‹œê°í™”\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "        fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "        colors = plt.get_cmap('tab10', len(pivot_df.columns))\n",
    "        pivot_df.plot(kind='barh', stacked=True, ax=ax, color=colors.colors, width=0.8)\n",
    "\n",
    "        ax.set_title(f'ì¢…í•© ì ìˆ˜ ìƒìœ„ {self.top_n}ê°œ ê¸°ì‚¬ì˜ ë…ìì¸µ ë¶„í¬ (ì—°ë ¹/ì„±ë³„)',\n",
    "                     fontsize=18, pad=20, fontdict=self.font_prop)\n",
    "        ax.set_xlabel('ë¹„ìœ¨ (%)', fontsize=12, fontdict=self.font_prop)\n",
    "        ax.set_ylabel('ê¸°ì‚¬ ID (ìƒìœ„ ìˆœ)', fontsize=12, fontdict=self.font_prop)\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "        legend = ax.legend(title='ë…ì ê·¸ë£¹', bbox_to_anchor=(1.02, 1),\n",
    "                           loc='upper left', borderaxespad=0.)\n",
    "        if self.font_name:\n",
    "            plt.setp(legend.get_texts(), fontfamily=self.font_name)\n",
    "            plt.setp(legend.get_title(), fontfamily=self.font_name)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "        plt.savefig(f'{self.result_dir}top_articles_demographics.png', dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"  - {self.result_dir}top_articles_demographics.png ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "    def _plot_demographics_heatmap(self, top_demographics, exclude_total=False):\n",
    "        \"\"\"ì¸êµ¬í†µê³„ íˆíŠ¸ë§µ\"\"\"\n",
    "        # ì „ì²´ ì œì™¸ ì˜µì…˜\n",
    "        if exclude_total:\n",
    "            top_demographics = top_demographics[\n",
    "                top_demographics['age_group'] != 'ì „ì²´'\n",
    "                ].copy()\n",
    "            filename = f'{self.result_dir}top_articles_demographics_heatmap_ì „ì²´ì œì™¸.png'\n",
    "        else:\n",
    "            filename = f'{self.result_dir}top_articles_demographics_heatmap.png'\n",
    "\n",
    "        top_demographics['demographic_group'] = (\n",
    "                top_demographics['age_group'] + \" \" + top_demographics['gender']\n",
    "        )\n",
    "\n",
    "        demographics_summary = top_demographics.groupby(\n",
    "            ['article_id', 'demographic_group']\n",
    "        )['ratio'].mean().reset_index()\n",
    "\n",
    "        # ë°ì´í„° í”¼ë²—\n",
    "        pivot_df = demographics_summary.pivot(\n",
    "            index='article_id', columns='demographic_group', values='ratio'\n",
    "        ).fillna(0)\n",
    "        pivot_df = pivot_df.loc[self.top_article_ids]\n",
    "\n",
    "        # íˆíŠ¸ë§µ ì‹œê°í™”\n",
    "        fig, ax = plt.subplots(figsize=(20, 12))\n",
    "\n",
    "        sns.heatmap(\n",
    "            pivot_df,\n",
    "            annot=True,\n",
    "            fmt=\".1f\",\n",
    "            cmap=\"viridis\",\n",
    "            linewidths=.5,\n",
    "            annot_kws={\"size\": 8},\n",
    "            ax=ax\n",
    "        )\n",
    "\n",
    "        ax.set_title(f'ì¢…í•© ì ìˆ˜ ìƒìœ„ {self.top_n}ê°œ ê¸°ì‚¬ì˜ ë…ìì¸µ ë¶„í¬ íˆíŠ¸ë§µ', fontsize=20, pad=20, fontdict=self.font_prop)\n",
    "        ax.set_xlabel('ë…ì ê·¸ë£¹ (ì—°ë ¹/ì„±ë³„)', fontsize=12, fontdict=self.font_prop)\n",
    "        ax.set_ylabel('ê¸°ì‚¬ ID (ìƒìœ„ ìˆœ)', fontsize=12, fontdict=self.font_prop)\n",
    "\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", fontfamily=self.font_name)\n",
    "        plt.setp(ax.get_yticklabels(), rotation=0, fontfamily=self.font_name)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filename, dpi=300)\n",
    "        plt.close()\n",
    "        print(f\"  - {filename} ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "    def analyze_wordcloud(self):\n",
    "        \"\"\"3. ì›Œë“œí´ë¼ìš°ë“œ ë¶„ì„\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"3. ì›Œë“œí´ë¼ìš°ë“œ ë¶„ì„ ì‹œì‘\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # ìƒìœ„ Nê°œ ê¸°ì‚¬ì˜ ë³¸ë¬¸ ë‚´ìš© í•„í„°ë§\n",
    "        top_contents_df = self.contents_df[\n",
    "            self.contents_df['article_id'].isin(self.top_article_ids)\n",
    "        ]\n",
    "\n",
    "        # ëª¨ë“  ë³¸ë¬¸ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°\n",
    "        combined_content = \" \".join(top_contents_df['content'].astype(str))\n",
    "\n",
    "        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±\n",
    "        font_path = get_font_path()\n",
    "        if not font_path:\n",
    "            print(\"ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒì„±í•˜ê¸° ìœ„í•œ í•œê¸€ í°íŠ¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            wordcloud, top_keywords = self._generate_wordcloud_and_counts(\n",
    "                combined_content, font_path, top_n=self.top_n\n",
    "            )\n",
    "\n",
    "            # ì›Œë“œí´ë¼ìš°ë“œ ì‹œê°í™” ë° ì €ì¥\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.title(f'ì¢…í•© ì ìˆ˜ ìƒìœ„ {self.top_n}ê°œ ê¸°ì‚¬ í•µì‹¬ í‚¤ì›Œë“œ', size=20, pad=20, fontdict=self.font_prop)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout(pad=0)\n",
    "            plt.savefig(f'{self.result_dir}top_articles_wordcloud.png', dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"  - {self.result_dir}top_articles_wordcloud.png ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "            print(f\"\\n--- ì¢…í•© ì ìˆ˜ ìƒìœ„ {self.top_n}ê°œ ê¸°ì‚¬ í•µì‹¬ í‚¤ì›Œë“œ TOP {self.top_n} ---\")\n",
    "            for i, (word, count) in enumerate(top_keywords, 1):\n",
    "                print(f\"{i:2d}ìœ„: {word} (ì–¸ê¸‰ íšŸìˆ˜: {count})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            print(\"konlpyì™€ Java(JDK)ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "        print(\"ì›Œë“œí´ë¼ìš°ë“œ ë¶„ì„ ì™„ë£Œ\\n\")\n",
    "\n",
    "    def _generate_wordcloud_and_counts(self, text_data, font_path, top_n):\n",
    "        \"\"\"ì›Œë“œí´ë¼ìš°ë“œ ê°ì²´ì™€ ìƒìœ„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "        results = self.kiwi.analyze(text_data)\n",
    "        nouns = [token.form for token in results[0][0] if token.tag in {'NNG', 'NNP'}]\n",
    "\n",
    "        # ë¶ˆìš©ì–´ ì²˜ë¦¬\n",
    "        stopwords = get_stopwords()\n",
    "        filtered_nouns = [word for word in nouns if len(word) > 1 and word not in stopwords]\n",
    "        word_counts = Counter(filtered_nouns)\n",
    "\n",
    "        wc = WordCloud(\n",
    "            font_path=font_path,\n",
    "            background_color='white',\n",
    "            width=1000,\n",
    "            height=800,\n",
    "            max_words=100,\n",
    "            colormap='viridis'\n",
    "        ).generate_from_frequencies(word_counts)\n",
    "\n",
    "        # ìƒìœ„ Nê°œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "        top_keywords = word_counts.most_common(top_n)\n",
    "\n",
    "        return wc, top_keywords\n",
    "\n",
    "    def analyze_topics_lda(self):\n",
    "        \"\"\"4. LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„\"\"\"\n",
    "        print(\"=\" * 50)\n",
    "        print(\"4. LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ì‹œì‘\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # ìƒìœ„ Nê°œ ê¸°ì‚¬ì˜ ë³¸ë¬¸ ë‚´ìš© í•„í„°ë§\n",
    "        top_contents_df = self.contents_df[\n",
    "            self.contents_df['article_id'].isin(self.top_article_ids)\n",
    "        ].copy()\n",
    "\n",
    "        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
    "        stopwords = get_stopwords()\n",
    "\n",
    "        processed_docs = []\n",
    "        for doc in top_contents_df['content'].astype(str):\n",
    "            results = self.kiwi.analyze(doc)\n",
    "            nouns = [token.form for token in results[0][0] if token.tag in {'NNG', 'NNP'}]\n",
    "\n",
    "            filtered_nouns = [word for word in nouns if len(word) > 1 and word not in stopwords]\n",
    "            processed_docs.append(\" \".join(filtered_nouns))\n",
    "\n",
    "        top_contents_df['processed_content'] = processed_docs\n",
    "\n",
    "        # TF-IDF ë²¡í„°í™”\n",
    "        vectorizer = TfidfVectorizer(max_df=0.85, min_df=2)\n",
    "        doc_term_matrix = vectorizer.fit_transform(top_contents_df['processed_content'])\n",
    "\n",
    "        # LDA ëª¨ë¸ í•™ìŠµ\n",
    "        NUM_TOPICS = 3\n",
    "        lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, random_state=42)\n",
    "        lda_model.fit(doc_term_matrix)\n",
    "\n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        self._display_topics(lda_model, feature_names, 10)\n",
    "\n",
    "        # ê° ê¸°ì‚¬ë³„ í† í”½ í• ë‹¹\n",
    "        topic_distributions = lda_model.transform(doc_term_matrix)\n",
    "        top_contents_df['topic_num'] = topic_distributions.argmax(axis=1)\n",
    "\n",
    "        # í† í”½ë³„ ê¸°ì‚¬ ëª©ë¡ ì¶œë ¥\n",
    "        print(\"\\n--- í† í”½ë³„ ê¸°ì‚¬ ëª©ë¡ ---\")\n",
    "        for i in range(NUM_TOPICS):\n",
    "            topic_articles = top_contents_df[top_contents_df['topic_num'] == i]\n",
    "\n",
    "            # í† í”½ í‚¤ì›Œë“œ\n",
    "            topic_keywords = \" \".join([\n",
    "                feature_names[j] for j in lda_model.components_[i].argsort()[:-10 - 1:-1]\n",
    "            ])\n",
    "            print(f\"\\n[ í† í”½ #{i + 1}: {topic_keywords} ]\")\n",
    "\n",
    "            if not topic_articles.empty:\n",
    "                for index, row in topic_articles.iterrows():\n",
    "                    article_id = row['article_id']\n",
    "                    title = row['title']\n",
    "                    print(f\"  - (ID: {article_id}) {title}\")\n",
    "            else:\n",
    "                print(\"  - ì´ í† í”½ì— í•´ë‹¹í•˜ëŠ” ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "        print(\"\\nLDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ì™„ë£Œ\\n\")\n",
    "\n",
    "    def _display_topics(self, model, feature_names, no_top_words):\n",
    "        \"\"\"LDA ëª¨ë¸ì˜ í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ë¥¼ ì¶œë ¥\"\"\"\n",
    "        print(\"\\n--- LDA í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ---\")\n",
    "        for topic_idx, topic in enumerate(model.components_):\n",
    "            top_words = \" \".join([\n",
    "                feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]\n",
    "            ])\n",
    "            print(f\"í† í”½ #{topic_idx + 1}: {top_words}\")\n",
    "\n",
    "    def run_all_analyses(self):\n",
    "        \"\"\"ëª¨ë“  ë¶„ì„ ì‹¤í–‰\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ë‰´ìŠ¤ ê¸°ì‚¬ í†µí•© ë¶„ì„ ì‹œì‘\")\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "        # 1. ê¸°ì‚¬ ì§€í‘œ ë¶„ì„\n",
    "        self.analyze_metrics()\n",
    "\n",
    "        # 2. ì¸êµ¬í†µê³„ ë¶„ì„\n",
    "        self.analyze_demographics()\n",
    "\n",
    "        # 3. ì›Œë“œí´ë¼ìš°ë“œ ë¶„ì„\n",
    "        self.analyze_wordcloud()\n",
    "\n",
    "        # 4. LDA í† í”½ ëª¨ë¸ë§\n",
    "        self.analyze_topics_lda()\n",
    "\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "        print(\"=\" * 50 + \"\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    analyzer = NewsAnalyzer(data_dir='./data/')\n",
    "\n",
    "    # ì „ì²´ ë¶„ì„ ì‹¤í–‰\n",
    "    analyzer.run_all_analyses()\n",
    "\n",
    "    # ë˜ëŠ” ê°œë³„ ë¶„ì„ ì‹¤í–‰ ê°€ëŠ¥\n",
    "    # analyzer.analyze_metrics()\n",
    "    # analyzer.analyze_demographics()\n",
    "    # analyzer.analyze_wordcloud()\n",
    "    # analyzer.analyze_topics_lda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "734a3266-28ea-4aad-9bf4-bd9b7b455c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì„±ê³µ: 'Malgun Gothic' í°íŠ¸ê°€ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "ì„±ëŠ¥ ìµœì í™”: './data/article_metrics_monthly.feather' íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "ì„±ëŠ¥ ìµœì í™”: './data/demographics_part001.feather' íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "ì„±ëŠ¥ ìµœì í™”: './data/demographics_part002.feather' íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.\n",
      "ëª¨ë“  ë°ì´í„° íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ë¶„ì„ê¸° ì‹¤í–‰ ë° ë°ì´í„° ë¡œë“œ\n",
    "data_dir = './data/' # ë°ì´í„° ê²½ë¡œë¡œ ì„¤ì • !\n",
    "\n",
    "analyzer = NewsAnalyzer(data_dir=data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2d91f3a-5bcf-4834-90c4-c230fc012b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "1. ê¸°ì‚¬ ì§€í‘œ ë¶„ì„ ì‹œì‘\n",
      "==================================================\n",
      "  - ./result/article_metrics_summary_raw.png ì €ì¥ ì™„ë£Œ\n",
      "  - ./result/article_metrics_summary_normalized.png ì €ì¥ ì™„ë£Œ\n",
      "ê¸°ì‚¬ ì§€í‘œ ë¶„ì„ ì™„ë£Œ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. ê¸°ì‚¬ ì§€í‘œ ë¶„ì„\n",
    "analyzer.analyze_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dfbb9ea7-2a9e-4121-a960-b188d3c8f2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "2. ì¸êµ¬í†µê³„ ë¶„ì„ ì‹œì‘\n",
      "==================================================\n",
      "  - ./result/top_articles_demographics.png ì €ì¥ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 45224 (\\N{HANGUL SYLLABLE NAM}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 50668 (\\N{HANGUL SYLLABLE YEO}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 51204 (\\N{HANGUL SYLLABLE JEON}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 52404 (\\N{HANGUL SYLLABLE CE}) missing from current font.\n",
      "  fig.canvas.draw()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - ./result/top_articles_demographics_heatmap.png ì €ì¥ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 45224 (\\N{HANGUL SYLLABLE NAM}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\seaborn\\utils.py:61: UserWarning: Glyph 50668 (\\N{HANGUL SYLLABLE YEO}) missing from current font.\n",
      "  fig.canvas.draw()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - ./result/top_articles_demographics_heatmap_ì „ì²´ì œì™¸.png ì €ì¥ ì™„ë£Œ\n",
      "ì¸êµ¬í†µê³„ ë¶„ì„ ì™„ë£Œ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. ì¸êµ¬í†µê³„ ë¶„ì„\n",
    "analyzer.analyze_demographics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d2596f50-5fd2-4e8d-b52b-b68624d747b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "3. ì›Œë“œí´ë¼ìš°ë“œ ë¶„ì„ ì‹œì‘\n",
      "==================================================\n",
      "  - ./result/top_articles_wordcloud.png ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "--- ì¢…í•© ì ìˆ˜ ìƒìœ„ 20ê°œ ê¸°ì‚¬ í•µì‹¬ í‚¤ì›Œë“œ TOP 20 ---\n",
      " 1ìœ„: ë“œë¼ë§ˆ (ì–¸ê¸‰ íšŸìˆ˜: 131)\n",
      " 2ìœ„: ì´ìš© (ì–¸ê¸‰ íšŸìˆ˜: 122)\n",
      " 3ìœ„: ìˆí¼ (ì–¸ê¸‰ íšŸìˆ˜: 120)\n",
      " 4ìœ„: ì„œë¹„ìŠ¤ (ì–¸ê¸‰ íšŸìˆ˜: 76)\n",
      " 5ìœ„: ì‹œì¥ (ì–¸ê¸‰ íšŸìˆ˜: 75)\n",
      " 6ìœ„: í”Œë«í¼ (ì–¸ê¸‰ íšŸìˆ˜: 71)\n",
      " 7ìœ„: ì†Œì…œ (ì–¸ê¸‰ íšŸìˆ˜: 71)\n",
      " 8ìœ„: ì¡°ì‚¬ (ì–¸ê¸‰ íšŸìˆ˜: 67)\n",
      " 9ìœ„: ì±„ë„ (ì–¸ê¸‰ íšŸìˆ˜: 66)\n",
      "10ìœ„: ì„¸ëŒ€ (ì–¸ê¸‰ íšŸìˆ˜: 64)\n",
      "11ìœ„: íŒ©íŠ¸ (ì–¸ê¸‰ íšŸìˆ˜: 58)\n",
      "12ìœ„: ì²´í¬ (ì–¸ê¸‰ íšŸìˆ˜: 57)\n",
      "13ìœ„: ë³´ë„ (ì–¸ê¸‰ íšŸìˆ˜: 52)\n",
      "14ìœ„: í•œêµ­ (ì–¸ê¸‰ íšŸìˆ˜: 51)\n",
      "15ìœ„: ì‹ ë¬¸ (ì–¸ê¸‰ íšŸìˆ˜: 51)\n",
      "16ìœ„: ì´ìš©ì (ì–¸ê¸‰ íšŸìˆ˜: 50)\n",
      "17ìœ„: ì •ë³´ (ì–¸ê¸‰ íšŸìˆ˜: 48)\n",
      "18ìœ„: êµìœ¡ (ì–¸ê¸‰ íšŸìˆ˜: 48)\n",
      "19ìœ„: ë¯¸êµ­ (ì–¸ê¸‰ íšŸìˆ˜: 47)\n",
      "20ìœ„: ì¤‘êµ­ (ì–¸ê¸‰ íšŸìˆ˜: 46)\n",
      "ì›Œë“œí´ë¼ìš°ë“œ ë¶„ì„ ì™„ë£Œ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. í‚¤ì›Œë“œ ë¶„ì„ (ì›Œë“œí´ë¼ìš°ë“œ)\n",
    "analyzer.analyze_wordcloud()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "017b2816-cde2-4647-9cb5-9b05f826ad21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "4. LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ì‹œì‘\n",
      "==================================================\n",
      "\n",
      "--- LDA í† í”½ ëª¨ë¸ë§ ê²°ê³¼ ---\n",
      "í† í”½ #1: ë³´ë„ ê²½ì œ ì–¼êµ´ ì´ìš© ì´ë²¤íŠ¸ ëª…ì˜ˆ ì¡°ì‚¬ ì°¸ì—¬ êµìœ¡ ììœ \n",
      "í† í”½ #2: ê³ í†µ ì„¸ëŒ€ ì•„ì´ ë„·í”Œë¦­ìŠ¤ ì €ë„ë¦¬ì¦˜ íŒ€ì¥ ì§€ë§ ë¬¸ì œ êµìœ¡ í¬í•¨\n",
      "í† í”½ #3: ë“œë¼ë§ˆ ìˆí¼ ì±„ë„ ì‹œì¥ ì†Œì…œ ì—°ë§ ì„œë¹„ìŠ¤ íŒŸìºìŠ¤íŠ¸ í”Œë«í¼ ì—°ì• \n",
      "\n",
      "--- í† í”½ë³„ ê¸°ì‚¬ ëª©ë¡ ---\n",
      "\n",
      "[ í† í”½ #1: ë³´ë„ ê²½ì œ ì–¼êµ´ ì´ìš© ì´ë²¤íŠ¸ ëª…ì˜ˆ ì¡°ì‚¬ ì°¸ì—¬ êµìœ¡ ììœ  ]\n",
      "  - (ID: 223338139549) [<2023 ì–¸ë¡ ìˆ˜ìš©ì ì¡°ì‚¬> ê²°ê³¼] ëŒ€ë¶€ë¶„ ë§¤ì²´ì—ì„œ ë‰´ìŠ¤ ì´ìš©ë¥  ê°ì†Œ ì–¸ë¡ Â·ì–¸ë¡ ì¸ì— ëŒ€í•œ í‰ê°€ë„ í•˜ë½\n",
      "  - (ID: 223367981164) [í”„ë‘ìŠ¤] ì„ ì¶œì§ ê³µë¬´ì›  ëª…ì˜ˆí›¼ì† ê³µì†Œì‹œíš¨ ì—°ì¥,  ì–¸ë¡  ììœ  ì¹¨í•´ì¼ê¹Œ?\n",
      "  - (ID: 223428024936) ã€Šì‹ ë¬¸ê³¼ë°©ì†¡ã€‹ ë°œê°„ 60ì£¼ë…„ ê¸°ë… ì´ë²¤íŠ¸ ğŸ€\n",
      "  - (ID: 223430762932) [ì–¸ë¡ ì‚¬ ì´íƒˆí•˜ëŠ” ì£¼ë‹ˆì–´ ê¸°ì ì‹¤íƒœ] í•œ ëª… í•œ ëª…ì´ ì†Œì¤‘í•œ ìì›,  ë”ëŠ” íšŒì‚¬ ë“±ì§€ì§€ ì•Šë„ë¡\n",
      "  - (ID: 223435080045) [íŒ©íŠ¸ì²´í¬ í˜„í™© ì ê²€] â€˜ì„ ê±°â€™ì—ì„œ â€˜íƒ•í›„ë£¨â€™ê¹Œì§€ ì•„ì´í…œ ì„ ì •ì´ â€˜íŒ©íŠ¸ì²´í¬â€™ ì˜ ê´€ê±´\n",
      "  - (ID: 223498456595) [EBS ë‹¤íí”„ë¼ì„ <ëˆì˜ ì–¼êµ´> ì œì‘ê¸°] â€˜ê²½ì•Œëª»â€™ì„ ìœ„í•œ ê²½ì œ êµìœ¡ ë‹¤íë©˜í„°ë¦¬\n",
      "\n",
      "[ í† í”½ #2: ê³ í†µ ì„¸ëŒ€ ì•„ì´ ë„·í”Œë¦­ìŠ¤ ì €ë„ë¦¬ì¦˜ íŒ€ì¥ ì§€ë§ ë¬¸ì œ êµìœ¡ í¬í•¨ ]\n",
      "  - (ID: 223338149961) [MZì„¸ëŒ€ê°€ ë§í•˜ëŠ” ë‰´ìŠ¤ ì´ìš©ë²•] â€œìš°ë¦¬ê°€ ë‰´ìŠ¤ë¥¼ ì•ˆ ë³¸ë‹¤ê³ ?â€ ì˜¨ë¼ì¸ ì»¤ë®¤ë‹ˆí‹°ì™€ ìˆí¼ í†µí•´ ë‰´ìŠ¤ ë³¸ë‹¤\n",
      "  - (ID: 223410633962) [ë¶ë¦¬ë·°: ã€Ší•œêµ­ì˜ ê¸°ìã€‹] ì§ì—…ìœ¼ë¡œì„œì˜ ê¸°ì, ì–´ì œì™€ ì˜¤ëŠ˜ ê·¸ë¦¬ê³  ë¯¸ë˜\n",
      "  - (ID: 223430759166) [ì–¸ë¡ ì‚¬ ì±„ìš©, í˜„í™©ê³¼ ë¬¸ì œì  ì§„ë‹¨] ë…¼ìˆ ë¡œ ë½‘ëŠ” ì–¸ë¡ ì¸? ì·¨ì¬ì™€ ê¸°ì‚¬ ì‘ì„± ëŠ¥ë ¥ì´ ë¨¼ì €ë‹¤\n",
      "  - (ID: 223435090740) [ë¶ë¦¬ë·°: ã€Šê³ í†µ êµ¬ê²½í•˜ëŠ” ì‚¬íšŒã€‹] êµ¬ê²½ê³¼ ëª©ê²© ì‚¬ì´ ê·¸ ì–´ë”˜ê°€  ë‰´ìŠ¤ì˜ í˜„ì¬ ìœ„ì¹˜ë¥¼ ìƒê°í•œë‹¤\n",
      "  - (ID: 223468670006) [ì œë³´íŒ€ì¥ì˜ ë“±ì¥ê³¼ ì–¸ë¡  ìœ¤ë¦¬] ê¸°ì‚¬ì™€ ê´‘ê³  ì‚¬ì´ ì¤„íƒ€ê¸° í•œ â€˜ì œë³´íŒ€ì¥â€™,  ë²•ì˜ í—ˆì  íŒŒê³ ë“¤ì–´\n",
      "  - (ID: 223848899706) [ê¸°ìë“¤ì´ ë§í•˜ëŠ” ì›Œë¼ë°¸] ì¼ê³¼ ì‚¶ ì‚¬ì´, ë‚˜ì˜ ì˜¤ëŠ˜ì€\n",
      "  - (ID: 223921751987) [ë„·í”Œë¦­ìŠ¤ ë‚´ ì…ì§€ ì¤„ì–´ë“œëŠ” ë°©ì†¡ì‚¬ ì½˜í…ì¸ ] ìµœëŒ€ ì‹œì²­ì í™•ë³´ë¥¼ ìœ„í•œ ìƒˆë¡œìš´  ì°½êµ¬(windowing) ì „ëµì„ ìˆ˜ë¦½í•´ì•¼\n",
      "\n",
      "[ í† í”½ #3: ë“œë¼ë§ˆ ìˆí¼ ì±„ë„ ì‹œì¥ ì†Œì…œ ì—°ë§ ì„œë¹„ìŠ¤ íŒŸìºìŠ¤íŠ¸ í”Œë«í¼ ì—°ì•  ]\n",
      "  - (ID: 223147765898) [íƒœí’ì´ ëœ ë¬´ë£Œ OTT: FAST] â€˜ë¬´ë£Œâ€™ì™€ â€˜ì½˜í…ì¸  ë‹¤ì–‘ì„±â€™ìœ¼ë¡œ  ì¼€ì´ë¸”TV ë°€ì–´ë‚¸ ëŒ€ì„¸, í•œêµ­ì—ì„œë„ ì„±ê³µí• ê¹Œ\n",
      "  - (ID: 223276858394) [AIë¡œ í—ˆë¬¼ì–´ì§€ëŠ” ì–¸ì–´ ì¥ë²½] íŒŸìºìŠ¤íŠ¸ì— AI ë„ì…í•œ ìŠ¤í¬í‹°íŒŒì´, ì½˜í…ì¸  ê¸€ë¡œë²Œ ê²½ìŸì˜ ì‹ í˜¸íƒ„ ë ê¹Œ?\n",
      "  - (ID: 223338373790) [ì¤‘êµ­] 20ë¶„ë„ ì•ˆ ë˜ëŠ” ì›¹ë“œë¼ë§ˆê°€  ì¼ì£¼ì¼ ë§Œì— 3ì–µ ë·°? ì¤‘êµ­ ìˆí¼ ì›¹ë“œë¼ë§ˆì˜ í­ë°œì ì¸ ì„±ì¥\n",
      "  - (ID: 223530772568) [ì¤‘êµ­ ìˆí¼ ë“œë¼ë§ˆ ì‹œì¥ì˜ í­ë°œì  ì„±ì¥] ìˆí¼ ë“œë¼ë§ˆëŠ” ì½˜í…ì¸  ì‹œì¥ í™œë ¥ì†Œ?  êµ­ë‚´ ì—…ê³„ë„ í”Œë«í¼ ìœ¡ì„±ì— ì£¼ëª©í•´ì•¼\n",
      "  - (ID: 223570656420) [K-ì½˜í…ì¸ ì— ìŠ¤ë©°ë“œëŠ” ìƒ¤ë¨¸ë‹ˆì¦˜ ì—´í’] ì „í†µë¬¸í™” ì €ë³€ì— ê¹”ë¦° ìµìˆ™í•œ ì½”ë“œ  â€˜í™â€™í•œ ê°œì„± ì…ê³  ë¶ˆì•ˆí•œ MZì„¸ëŒ€ íŒŒê³ ë“¤ì–´\n",
      "  - (ID: 223688762248) ã€Šì‹ ë¬¸ê³¼ë°©ì†¡ã€‹ ì—°ë§ ê¸°ë… ì´ë²¤íŠ¸ â˜ƒï¸\n",
      "  - (ID: 223746016915) [<2024 ì†Œì…œë¯¸ë””ì–´ ì´ìš©ì ì¡°ì‚¬> ê²°ê³¼] ê°™ì€ ë“¯ ì„œë¡œ ë‹¤ë¥¸ ì†Œì…œë¯¸ë””ì–´,  ì´ìš©ì íŠ¹ì„±ê³¼ ìš©ë„ ë”°ë¼ ë‹¤ì±„ë¡­ê²Œ ì§„í™”\n",
      "\n",
      "LDA í† í”½ ëª¨ë¸ë§ ë¶„ì„ ì™„ë£Œ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. í† í”½ ëª¨ë¸ë§ (LDA)\n",
    "analyzer.analyze_topics_lda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
